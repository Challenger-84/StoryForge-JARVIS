{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda\n",
      "1 epoch = 1 batches\n",
      "step 10, loss: 1.2398115396499634, dt: 431.26 ms, tok/sec 4748.850560440502\n",
      "step 20, loss: 0.4794187843799591, dt: 432.53 ms, tok/sec 4734.971637910268\n",
      "step 30, loss: 0.23416724801063538, dt: 432.59 ms, tok/sec 4734.240943284023\n",
      "step 40, loss: 0.3205746114253998, dt: 437.76 ms, tok/sec 4678.363895609058\n",
      "step 50, loss: 0.15725810825824738, dt: 467.58 ms, tok/sec 4380.034128833133\n",
      "step 60, loss: 0.22025199234485626, dt: 456.41 ms, tok/sec 4487.203584775361\n",
      "step 70, loss: 0.15782202780246735, dt: 628.19 ms, tok/sec 3260.1611693251034\n",
      "step 80, loss: 0.08541318029165268, dt: 451.24 ms, tok/sec 4538.594510742127\n",
      "step 90, loss: 0.021308371797204018, dt: 525.79 ms, tok/sec 3895.1213990258043\n",
      "step 100, loss: 0.0583316832780838, dt: 486.14 ms, tok/sec 4212.759506980777\n"
     ]
    }
   ],
   "source": [
    "#-------------------------! import statements !-------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "#-------------------------! Hyperparameter !-------------------------\n",
    "@dataclass\n",
    "class BartConfig:\n",
    "    model_name: str = \"facebook/bart-base\"\n",
    "    batch_size: int = 2\n",
    "    block_size: int = 1024 \n",
    "\n",
    "#-------------------------! Load BART Model and Tokenizer !-------------------------\n",
    "config = BartConfig()\n",
    "tokenizer = BartTokenizer.from_pretrained(config.model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(config.model_name)\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f'using device {device}')\n",
    "\n",
    "model.to(device)\n",
    "model = torch.compile(model, backend='eager')\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "#-------------------------! Data Loader Class !-------------------------\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, tokenizer):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        with open('../input.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "        tokens = tokenizer(text, return_tensors=\"pt\", max_length=T, truncation=True, padding=\"max_length\")[\"input_ids\"]\n",
    "        self.tokens = tokens.to(device)\n",
    "\n",
    "        self.num_batches = max(len(self.tokens) // B, 1)\n",
    "        print(f'1 epoch = {self.num_batches} batches')\n",
    "\n",
    "        # state\n",
    "        self.current_position = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        start_idx = self.current_position\n",
    "        end_idx = start_idx + B\n",
    "\n",
    "        x = self.tokens[start_idx:end_idx]\n",
    "        y = self.tokens[start_idx:end_idx]\n",
    "\n",
    "        self.current_position += B\n",
    "\n",
    "        if self.current_position + B > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "\n",
    "        return x, y\n",
    "\n",
    "train_loader = DataLoaderLite(config.batch_size, config.block_size, tokenizer)\n",
    "\n",
    "def train(model, train_loader, optimizer, num_epochs=100):\n",
    "    model.train()\n",
    "    for i in range(num_epochs):\n",
    "        t0 = time.time()\n",
    "        src, tgt = train_loader.next_batch()\n",
    "        optimizer.zero_grad()\n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            output = model(input_ids=src, labels=tgt)\n",
    "            loss = output.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.cuda.synchronize()\n",
    "        t1 = time.time()\n",
    "        dt = (t1 - t0) * 1000  # time difference is in milliseconds\n",
    "        tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f'step {i + 1}, loss: {loss.item()}, dt: {dt:.2f} ms, tok/sec {tokens_per_sec}')\n",
    "\n",
    "train(model, train_loader, optimizer, num_epochs=100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
